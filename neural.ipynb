{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Importações gerais e Funções\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "pd.set_option('display.max_rows', 250)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# ------ Funções\n",
    "\n",
    "# Função para manipulação das datas\n",
    "def separa_datas(df, col):\n",
    "    temp = pd.DataFrame()\n",
    "    print('\\nResgatando informações do dataframe...')\n",
    "    temp[col] = pd.to_datetime(df[col], format='%b-%Y')\n",
    "#    dias = []  # cria lista vazia para ser preenchida com a informação dos dias\n",
    "    mes_ano = []  # cria lista vazia para ser preenchida com mês/ano\n",
    "\n",
    "    if temp[col].isnull().sum() > 0:\n",
    "        print(f'\\nATENÇÃO! Identificadas NaNs na feature {col}, impossível continuar.')\n",
    "        print(f'Favor resolver as {temp[col].isnull().sum()} NaNs antes de tentar novamente. Seu bosta.')\n",
    "    else:\n",
    "        print('\\nSeparando parâmetros de data e gerando nova série, só um instante por favor.')\n",
    "        for i in range(0, len(temp)):  # preenche a lista com mês e ano\n",
    "            mes_ano.append(temp.loc[i, col].strftime('%b-%Y').lower())\n",
    "        print('\\nTudo pronto.')\n",
    "\n",
    "        return pd.Series(data=mes_ano, name='mes_ano')  # retorna uma série nova\n",
    "\n",
    "\n",
    "\n",
    "# Função para buscar a explicação de cada feature no dicionário e analisar NaNs\n",
    "def analise_geral(lista, dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param lista: lista de features a se verificar\n",
    "    :dtype lista: list\n",
    "    :param dataframe: o banco de dados que contem todas as features que serão verificadas\n",
    "    :type dataframe: DataFrame\n",
    "    \"\"\"\n",
    "    def significado(x: object):\n",
    "        dic = pd.read_csv('./data/dictionary.csv', index_col=False)\n",
    "        print(f'Nome: {str(x).upper()}')\n",
    "        print(f'Definição: {dic[dic['Feature'] == str(x)]['Descrição'].values}')\n",
    "\n",
    "    if len(lista) > 1:\n",
    "        for i in range(0, len(lista)):\n",
    "            print('\\n')\n",
    "            print(f'Feature #{i+1}/{len(lista)}')\n",
    "            significado(lista[i])\n",
    "            print(f'Tipo de dado: {str(dataframe[lista[i]].dtypes).upper()}')\n",
    "            print(f'Quantidade total de NaNs: {dataframe[lista[i]].isnull().sum()}')\n",
    "            print(f'Proporção desses NaNs: {round((dataframe[lista[i]].isnull().sum()/len(dataframe))*100, 2)}%')\n",
    "    elif len(lista) == 1:\n",
    "        print('\\n')\n",
    "        significado(lista[0])\n",
    "        print(f'Tipo de dado: {str(dataframe[lista[0]].dtypes).upper()}')\n",
    "        print(f'Quantidade total de NaNs: {dataframe[lista[0]].isnull().sum()}')\n",
    "        print(f'Proporção desses NaNs: {round((dataframe[lista[0]].isnull().sum()/len(dataframe))*100, 2)}%')\n",
    "\n",
    "\n",
    "\n",
    "# Função que analisa apenas as features sem NaNs\n",
    "def analise_nans(lista, dataframe: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    :param lista: lista de features a se verificar\n",
    "    :dtype lista: list\n",
    "    :param dataframe: o banco de dados que contem todas as features que serão verificadas\n",
    "    :type dataframe: DataFrame\n",
    "    \"\"\"\n",
    "    def significado(x: object):\n",
    "        dic = pd.read_csv('./data/dictionary.csv', index_col=False)\n",
    "        print(f'Nome: {str(x).upper()}')\n",
    "        print(f'Definição: {dic[dic['Feature'] == str(x)]['Descrição'].values}')\n",
    "    print('\\nVerificando...')\n",
    "\n",
    "    if len(lista) > 1:\n",
    "        if any(dataframe[lista].isnull().sum() > 1) == False:\n",
    "            print('==> Sem NaNs nessa lista, parabéns.')\n",
    "        else:\n",
    "            print('Calculando tamanho da solicitação, só um instante por favor.')\n",
    "            tam = len(dataframe[lista].isnull().sum()[dataframe[lista].isnull().sum() > 0])\n",
    "            idx = 1\n",
    "            for i in range(0, len(lista)):\n",
    "                if dataframe[lista[i]].isnull().sum() == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    print('\\n')\n",
    "                    print(f'Feature #{idx}/{tam}')\n",
    "                    significado(lista[i])\n",
    "                    print(f'Tipo de dado: {str(dataframe[lista[i]].dtypes).upper()}')\n",
    "                    print(f'Quantidade total de NaNs: {dataframe[lista[i]].isnull().sum()}')\n",
    "                    print(f'Proporção desses NaNs: {round((dataframe[lista[i]].isnull().sum()/len(dataframe))*100, 2)}%')\n",
    "                    idx += 1\n",
    "    elif len(lista) == 1:\n",
    "        if dataframe[lista[0]].isnull().sum() == 0:\n",
    "            print('==> Sem NaNs nessa lista, parabéns.')\n",
    "        else:\n",
    "            print('\\n')\n",
    "            significado(lista[0])\n",
    "            print(f'Tipo de dado: {str(dataframe[lista[0]].dtypes).upper()}')\n",
    "            print(f'Quantidade total de NaNs: {dataframe[lista[0]].isnull().sum()}')\n",
    "            print(f'Proporção desses NaNs: {round((dataframe[lista[0]].isnull().sum()/len(dataframe))*100, 2)}%')\n",
    "\n",
    "\n",
    "\n",
    "# Função para armazenar os valores de média e desvio padrão das features, para normalização dos valores\n",
    "def parametros_zscore(df: pd.DataFrame, norm:list):\n",
    "    \"\"\"\n",
    "    Args: df = DataFrame contendo os dados que serão utilizados\n",
    "          norm = lista com a relação das features que precisam passar por normalização. Caso\n",
    "                 norm=False a função utiliza as features do DataFrame original\n",
    "\n",
    "    Returns: DataFrame contendo os valores de média e desvio padrão de cada feature; lista\n",
    "            contendo o nome das features utilizadas\n",
    "    \"\"\"\n",
    "    if norm:\n",
    "        pass\n",
    "    else:\n",
    "    # Normalização apenas das colunas originais, sem contar as categóricas (como 'fez_hardship', por exemplo)\n",
    "        norm = ['EFFR', 'expec6m', 'expec12m', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
    "            'term', 'int_rate', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'fico_range_low',\n",
    "            'fico_range_high', 'inq_last_6mths', 'mths_since_last_delinq', 'open_acc', 'pub_rec',\n",
    "            'revol_bal', 'revol_util', 'total_acc', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
    "            'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "            'collection_recovery_fee', 'last_pymnt_amnt', 'last_fico_range_high', 'last_fico_range_low',\n",
    "            'collections_12_mths_ex_med', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal',\n",
    "            'mths_since_rcnt_il', 'total_rev_hi_lim', 'inq_fi', 'acc_open_past_24mths', 'avg_cur_bal',\n",
    "            'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct',\n",
    "            'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc','mths_since_recent_bc',\n",
    "            'mths_since_recent_inq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats',\n",
    "            'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats',\n",
    "            'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq',\n",
    "            'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens', 'tot_hi_cred_lim', 'total_bal_ex_mort',\n",
    "            'total_bc_limit', 'total_il_high_credit_limit', 'tempo_total_tomador', 'tempo_consulta',\n",
    "            'orig_projected_additional_accrued_interest', 'deferral_term', 'hardship_amount', 'hardship_length',\n",
    "            'hardship_dpd', 'hardship_payoff_balance_amount', 'hardship_last_payment_amount', 'tempo_total_contrato',\n",
    "            'tempo_payment_plan', 'annual_inc_joint', 'dti_joint', 'revol_bal_joint', 'sec_app_fico_range_low',\n",
    "            'sec_app_fico_range_high', 'sec_app_inq_last_6mths', 'sec_app_mort_acc', 'sec_app_open_acc',\n",
    "            'sec_app_revol_util', 'sec_app_open_act_il', 'sec_app_num_rev_accts', 'sec_app_chargeoff_within_12_mths',\n",
    "            'sec_app_collections_12_mths_ex_med']\n",
    "\n",
    "    new_df = {}\n",
    "    for name in norm:\n",
    "        media = df[name].mean()\n",
    "        desvio = df[name].std()\n",
    "        new_df.update({name: {\n",
    "                            'Media': media,\n",
    "                            'DesvPadrao': desvio,\n",
    "                            }\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(new_df), norm\n",
    "\n",
    "\n",
    "# Função de ZScore personalizada\n",
    "def tcc_zscore(df:pd.DataFrame, norm:list):\n",
    "    from scipy import stats\n",
    "    \"\"\"\n",
    "    Args: df = DataFrame contendo os dados para normalização\n",
    "          norm = lista contendo o nome de cada feature para normalização\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('Iniciando processo de normalização.\\n')\n",
    "        for name in norm:\n",
    "            df[name] = stats.zscore(df[name])\n",
    "        print('Processo de normalização concluído com sucesso!')\n",
    "    except Exception as e:\n",
    "        print(f'O seguinte erro ocorreu durante a tentativa de padronização por ZScore: {e}. Favor verificar')\n",
    "\n",
    "\n",
    "# Função de carregamento (parcial ou integral) do banco de dados\n",
    "def carregamento_treino(percentual=1):\n",
    "    print('Carregando banco de dados de treino completo...')\n",
    "    try:\n",
    "        modelagem = pd.read_parquet('data/modelagem.parquet')\n",
    "    except Exception as e:\n",
    "        print(f'Erro no carregamento: {e}. Favor verificar.')\n",
    "        \n",
    "    # O PC não conseguia processar o arquivo inteiro no modelo de LDA, optei por testar com apenas as 70% observações iniciais\n",
    "    # O processamento com 70% produz resultado, com utilização de 100% CPU e 99% RAM durante o treino\n",
    "    if (percentual < 1) and (percentual > 0):\n",
    "        print('Realizando redução das observações.')\n",
    "        treino_manter, treino_deletar = train_test_split(modelagem, train_size=percentual, random_state=1)\n",
    "        print('Liberando memória.')\n",
    "        del treino_deletar, modelagem\n",
    "\n",
    "        print('Realizando separação variáveis dependente e independentes.')\n",
    "        target = ['default']\n",
    "        test_data = treino_manter[target]\n",
    "        train_data = treino_manter.drop(target, axis = 1)\n",
    "        del treino_manter\n",
    "        print('Carregamento do banco de dados concluído.')\n",
    "        \n",
    "    elif percentual > 0:\n",
    "        print('Banco de dados carregado com sucesso, separando variáveis dependente e independentes.')\n",
    "        target = ['default']\n",
    "        test_data = modelagem[target]\n",
    "        train_data = modelagem.drop(target, axis = 1)\n",
    "        del modelagem\n",
    "        print('Carregamento do banco de dados concluído.')\n",
    "    else:\n",
    "        print('Favor registrar um percentual em decimais, entre 0 (exclusive) e 1 (inclusive).')\n",
    "        test_data = None\n",
    "        train_data = None\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "# Análise das métricas e geração da matriz de confusão\n",
    "def analise_metricas(classif=None, X=pd.DataFrame, y=pd.Series):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - classif: o algoritmo utilizado (o objeto que processa o .fit)\n",
    "    - X: DataFrame (normalmente teste_x)\n",
    "    - y: Series (normalmente teste_y)\n",
    "    \"\"\"\n",
    "    if not classif:\n",
    "        print('Necessário treinamento do modelo antes de se poder calcular as métricas')\n",
    "    else:\n",
    "        # ------ Cálculo das métricas do modelo\n",
    "        print('Realizando a previsão dos valores do banco de dados e calculando as métricas de avaliação.\\n')\n",
    "        inicio_prev = timer()\n",
    "        ypred = classif.predict(X)  # previsão dos dados de teste\n",
    "        acc = accuracy_score(y, ypred)  # cálculo accuracy (eficiência geral do modelo)\n",
    "        sens = recall_score(y, ypred, pos_label=1)  # cálculo da sensitividade/recall (taxa de acerto dos eventos)\n",
    "        spec = recall_score(y, ypred, pos_label=0)  # cálculo da especificidade (taxa de acerto dos não-eventos)\n",
    "        prec = precision_score(y, ypred)  # taxa de acerto dos positivos totais (TP/(TP+FP))\n",
    "        fim_prev = timer()\n",
    "\n",
    "        print(f'Análise do modelo:\\n')\n",
    "        print(f'Valor métrica Accuracy:: {acc:.6f}')\n",
    "        print(f'Valor métrica Sensitividade (Recall): {sens:.6f}')\n",
    "        print(f'Valor métrica Especificidade: {spec:.6f}')\n",
    "        print(f'Valor métrica Precision: {prec:.6f}')\n",
    "        print(f'Tempo de cálculo {timedelta(seconds=fim_prev-inicio_prev)}.')\n",
    "        print('\\n')\n",
    "\n",
    "        # ------ Matriz de confusão\n",
    "        conf_matrix = metrics.confusion_matrix(y.default.values.astype('int'), ypred, labels=[1, 0])\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.heatmap(conf_matrix,\n",
    "                    fmt='d',\n",
    "                    annot=True,\n",
    "                    cmap='Oranges',\n",
    "                    xticklabels=['Inadimplentes', 'Em dia'],\n",
    "                    yticklabels=['Inadimplentes', 'Em dia'])\n",
    "        plt.title(f'Matriz de Confusão', fontsize=12)\n",
    "        plt.xlabel('Dados de teste', fontsize=12)\n",
    "        plt.ylabel('Dados previstos', fontsize=12)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa926b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import categorical_hinge\n",
    "\n",
    "\n",
    "try:\n",
    "    print('Iniciando timer geral e carregando banco de dados de treino.')\n",
    "    inicio_geral = timer()\n",
    "    train_data, test_data = carregamento_treino(0.7)\n",
    "    \n",
    "    print('Calculando dados de média e desvio padrão antes da padronização no banco de dados. Também registrando o nome das features utilizadas')\n",
    "    historico, norm = parametros_zscore(train_data, norm=False)\n",
    "    print('Informações calculadas, histórico mantido no objeto \"historico\".\\n')\n",
    "    tcc_zscore(train_data, norm)\n",
    "    print('Normalização do banco de dados realizada, prosseguindo.\\n')\n",
    "\n",
    "       \n",
    "    # ------ Separação dados de treino e teste\n",
    "    print('Realizando procedimentos de separação de amostras para treino do modelo.')\n",
    "    target = ['default']\n",
    "    tamanho_treino = 0.8\n",
    "    treino_x, teste_x, treino_y, teste_y = train_test_split(train_data, test_data, train_size=tamanho_treino, random_state=1)\n",
    "    print('Liberando memória...')\n",
    "    del train_data, test_data  # libera memória\n",
    "    print('Pronto.\\n')\n",
    "\n",
    "\n",
    "    # ------ Parâmetros do modelo\n",
    "    print('Estabelecendo parâmetros do algoritmo de treino.')\n",
    "    nome_modelo = datetime.datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    classif_nn = tf.keras.Sequential()\n",
    "    #modelo_dnn = keras.Sequential([normalizador])\n",
    "    classif_nn.add(tf.keras.layers.Dense(64, activation='relu', name='Input_Dense64'))\n",
    "    classif_nn.add(tf.keras.layers.Dropout(0.3, seed=1))\n",
    "    classif_nn.add(tf.keras.layers.Dense(64, activation='relu', name='Hidden1_Dense64'))\n",
    "    #classif_nn.add(tf.keras.layers.Dense(128, activation='relu', name='HiddenL3_Dense128'))\n",
    "    classif_nn.add(tf.keras.layers.Dense(1, name='Output_Dense1'))\n",
    "    classif_nn.compile(loss=categorical_hinge, \n",
    "                    optimizer='Adam',\n",
    "                    metrics=['categorical_hinge'])\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss',  # callback de early stopping\n",
    "                                            patience=3)\n",
    "\n",
    "\n",
    "\n",
    "    classif_nn.fit(treino_x,\n",
    "                            treino_y,\n",
    "                            epochs=3,\n",
    "                            validation_data=(teste_x, teste_y),\n",
    "                            batch_size=32,\n",
    "                            callbacks=[callback])\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Erro durante o processo de treinamento: {e}! Favor verificar.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
